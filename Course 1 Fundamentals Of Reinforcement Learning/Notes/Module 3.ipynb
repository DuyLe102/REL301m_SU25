{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "854f271a",
   "metadata": {},
   "source": [
    "# Markov Decision Processes (MDPsMDPs)\n",
    "\n",
    "## 1. Limitations of k-Armed Bandit\n",
    "- Only considers immediate decisions (no situational awareness)\n",
    "- Focuses solely on immediate rewards (ignores future consequences)\n",
    "- Example: Always taking highest-paying job now might limit future opportunities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15131ddb",
   "metadata": {},
   "source": [
    "\n",
    "### Key Things to Remember:\n",
    "1. **States (S_t):** \n",
    "   - \"Snapshot\" of the environment at time t\n",
    "   - Examples: chess board position, robot sensor readings\n",
    "\n",
    "2. **Actions (A_t):** \n",
    "   - What the agent decides to DO\n",
    "   - Can be discrete (left/right) or continuous (steering angle)\n",
    "\n",
    "3. **Transition Dynamics:**\n",
    "   - Defined by probability distribution:  \n",
    "      `p(s', r | s, a) = Pr(S_{t+1}=s', R_{t+1}=r | S_t=s, A_t=a)`\n",
    "   - Must satisfy:  \n",
    "      `‚àë‚àë p(s',r|s,a) = 1` for all s ‚àà ùì¢, a ‚àà ùìê(s)\n",
    "\n",
    "4. **Reward (R_{t+1}):**\n",
    "   - Immediate feedback signal\n",
    "   - Not always $ - could be delayed!\n",
    "   - Design tip: Reward ‚â† goal, but should guide toward goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d71b528",
   "metadata": {},
   "source": [
    "## 3. The Real Goal in RL\n",
    "- Maximize **cumulative future rewards**, not immediate payoffs  \n",
    "- Total return: `G_t = R_{t+1} + R_{t+2} + ... + R_T`  \n",
    "- Key insight: Sacrifice short-term gains for better long-term outcomes\n",
    "\n",
    "## 4. Reward Hypothesis (Cool Analogy!)\n",
    "| Approach              | Proverb                          | Limitation                     |\n",
    "|-----------------------|----------------------------------|--------------------------------|\n",
    "| Programming AI        | \"Give a man a fish...\"           | Needs explicit instructions    |\n",
    "| Supervised Learning   | \"Teach a man to fish...\"         | Requires labeled data          |\n",
    "| **Reinforcement Learning** | \"Give a taste of fish...\"    | Learns from experience        |\n",
    "\n",
    "### Reward Design Challenges\n",
    "- No natural \"currency\" for rewards\n",
    "- Complex/long-term goals\n",
    "- Dynamic environments\n",
    "- Risk vs reward tradeoffs\n",
    "\n",
    "## 5. Task Types Comparison\n",
    "|                       | Episodic Tasks                   | Continuing Tasks               |\n",
    "|-----------------------|----------------------------------|--------------------------------|\n",
    "| **Structure**         | Natural breaks (episodes)        | Never-ending                  |\n",
    "| **Termination**       | Ends at terminal state           | No terminal state             |\n",
    "| **Return Formula**    | `G_t = ‚àë_{k=1}^T R_{t+k}`       | Needs discounting (Œ≥)         |\n",
    "\n",
    "## 6. Handling Infinite Horizons\n",
    "### Discounted Returns\n",
    "`G_t = R_{t+1} + Œ≥R_{t+2} + Œ≥¬≤R_{t+3} + ...`  \n",
    "`  = ‚àë_{k=0}^‚àû Œ≥^k R_{t+k+1}`\n",
    "\n",
    "Why discount (0 ‚â§ Œ≥ < 1)?\n",
    "- Makes infinite sums finite\n",
    "- Values immediate rewards more\n",
    "- Models uncertainty about future\n",
    "\n",
    "### The Magic Recursion\n",
    "`G_t = R_{t+1} + Œ≥G_{t+1}`  \n",
    "*(This recursive relationship is FUNDAMENTAL to RL algorithms)*\n",
    "\n",
    "> This recursion enables dynamic programming solutions!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
