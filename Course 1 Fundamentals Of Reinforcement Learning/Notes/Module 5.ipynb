{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dynamic Programming\n",
        "\n",
        "## Overview\n",
        "- **Dynamic Programming (DP)** uses Bellman equations to define iterative algorithms for **policy evaluation** and **control**.\n",
        "- Idea: keep improving the current policy until no further improvement is possible → that means the policy is approximately optimal.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Initialization\n",
        "- For each state $s \\in \\mathcal{S}$:\n",
        "  - $V(s) \\in \\mathbb{R}$: state value, initialized arbitrarily\n",
        "  - $\\pi(s) \\in \\mathcal{A}$: policy action, initialized arbitrarily\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Iterative Policy Evaluation\n",
        "- Input: Policy $\\pi$ to be evaluated\n",
        "- Goal: Estimate $v_\\pi(s)$ for all states $s$\n",
        "\n",
        "### Algorithm:\n",
        "1. Initialize: $V \\leftarrow \\vec{0}$, $V' \\leftarrow \\vec{0}$\n",
        "2. Repeat until convergence:\n",
        "   - $\\Delta \\leftarrow 0$\n",
        "   - For each $s \\in \\mathcal{S}$:\n",
        "     $$V'(s) \\leftarrow \\sum_a \\pi(a|s) \\sum_{s',r} p(s', r | s, a)[r + \\gamma V(s')]$$\n",
        "     $$\\Delta \\leftarrow \\max(\\Delta, |V'(s) - V(s)|)$$\n",
        "   - Update: $V \\leftarrow V'$\n",
        "- When $\\Delta < \\theta$, stop and return $V \\approx v_\\pi$\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Policy Improvement\n",
        "- For each $s \\in \\mathcal{S}$:\n",
        "  - Store previous action: `old_action ← π(s)`\n",
        "  - Update policy:\n",
        "    $$\\pi(s) \\leftarrow \\arg\\max_a \\sum_{s', r} p(s', r | s, a)[r + \\gamma V(s')]$$\n",
        "  - If new action differs from old → policy is not stable\n",
        "- If policy is stable for all states → stop and return:\n",
        "  - $V \\approx v^*$ (optimal value)\n",
        "  - $\\pi \\approx \\pi^*$ (optimal policy)\n",
        "- Otherwise → go back to step 2\n",
        "\n",
        "---\n",
        "\n",
        "## Generalized Policy Iteration\n",
        "- Alternates between:\n",
        "  1. **Policy evaluation**\n",
        "  2. **Policy improvement**\n",
        "- Repeat until the policy converges.\n",
        "\n",
        "---\n",
        "\n",
        "## Value Iteration\n",
        "- Parameter: $\\theta > 0$ (convergence threshold)\n",
        "- Initialize $V(s)$ arbitrarily for all $s$, except $V(terminal) = 0$\n",
        "\n",
        "### Algorithm:\n",
        "Repeat until convergence:\n",
        "- $\\Delta \\leftarrow 0$\n",
        "- For each $s \\in \\mathcal{S}$:\n",
        "  - Store old value: $v \\leftarrow V(s)$\n",
        "  - Update:\n",
        "    $$V(s) \\leftarrow \\max_a \\sum_{s', r} p(s', r | s, a)[r + \\gamma V(s')]$$\n",
        "  - $\\Delta \\leftarrow \\max(\\Delta, |v - V(s)|)$\n",
        "- When $\\Delta < \\theta$ → stop\n",
        "\n",
        "After convergence, extract policy:\n",
        "$$\\pi(s) = \\arg\\max_a \\sum_{s', r} p(s', r | s, a)[r + \\gamma V(s')]$$\n",
        "\n",
        "---\n",
        "\n",
        "## Alternative Methods\n",
        "\n",
        "| Method              | Description                                                                 |\n",
        "|---------------------|-----------------------------------------------------------------------------|\n",
        "| Monte Carlo         | Averages value estimates over multiple episodes following policy $\\pi$      |\n",
        "| Bootstrapping       | Uses previous value estimates to improve current estimates                  |\n",
        "| Brute-force Search  | Evaluates every deterministic policy and selects the best one               |\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Markdown",
      "language": "markdown",
      "name": "markdown"
    },
    "language_info": {
      "name": "markdown"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
