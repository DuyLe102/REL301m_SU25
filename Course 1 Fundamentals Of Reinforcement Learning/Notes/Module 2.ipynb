{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e8390b2",
   "metadata": {},
   "source": [
    "# The K-Armed Bandit Problem\n",
    "\n",
    "## 1. Problem Overview\n",
    "- **Objective**: A decision-maker (agent) selects one of $k$ possible actions and receives a stochastic reward based on the chosen action\n",
    "- **Key characteristics**:\n",
    "  - Reward distributions for actions are initially unknown\n",
    "  - True action value: $q_*(a) = \\mathbb{E}[R_t | A_t = a]$\n",
    "  - Optimal goal: Identify the action with the highest expected value\n",
    "\n",
    "## 2. Estimating Action Values\n",
    "\n",
    "### 2.1. Sample-Average Method\n",
    "$$Q_t(a) = \\frac{\\sum_{i=1}^{t-1} R_i \\cdot \\mathbb{I}(A_i = a)}{N_t(a)}$$\n",
    "- $N_t(a)$: Number of times action $a$ was chosen before timestep $t$\n",
    "- $R_i$: Reward received at trial $i$\n",
    "\n",
    "### 2.2. Incremental Update Rule\n",
    "$$Q_{n+1} = Q_n + \\frac{1}{n} \\left( R_n - Q_n \\right)$$\n",
    "- **Advantage**: No need to store historical reward data\n",
    "- **Online adjustment**: Updates estimates based on temporal-difference error $(R_n - Q_n)$\n",
    "\n",
    "## 3. Balancing Exploration & Exploitation\n",
    "\n",
    "### 3.1. The Fundamental Tradeoff\n",
    "| **Exploration**                        | **Exploitation**                  |\n",
    "|----------------------------------------|-----------------------------------|\n",
    "| Trying new actions to gather information | Choosing best-known action       |\n",
    "| Improves long-term knowledge           | Maximizes immediate rewards      |\n",
    "| **Mutually exclusive at each step**    |                                   |\n",
    "\n",
    "### 3.2. Îµ-Greedy Action Selection\n",
    "$$A_t = \\begin{cases} \n",
    "\\underset{a}{\\operatorname{argmax}} Q_t(a) & \\text{probability } 1-\\varepsilon \\\\\n",
    "\\text{random action from } \\{a_1, ..., a_k\\} & \\text{probability } \\varepsilon \n",
    "\\end{cases}$$\n",
    "- $\\varepsilon$: Exploration rate (typically 0.01-0.1)\n",
    "- **Balance**: Exploits current knowledge + explores randomly\n",
    "\n",
    "### 3.3. Optimistic Initial Values\n",
    "- **Mechanism**: Initialize $Q_0(a)$ higher than true values\n",
    "- **Effect**: Encourages early exploration of all actions\n",
    "- **Limitations**:\n",
    "  - Only effective during initial phases\n",
    "  - Unsuitable for non-stationary problems\n",
    "  - No systematic way to set initial values\n",
    "\n",
    "### 3.4. Upper Confidence Bound (UCB) Action Selection\n",
    "$$A_t = \\underset{a}{\\operatorname{argmax}} \\left[ Q_t(a) + c \\sqrt{\\frac{\\ln t}{N_t(a)}} \\right]$$\n",
    "- **Components**:\n",
    "  - $Q_t(a)$: Current value estimate\n",
    "  - $\\sqrt{\\frac{\\ln t}{N_t(a)}}$: Uncertainty measure\n",
    "  - $c$: Exploration control parameter\n",
    "- **Principle**: Prefers high-value or under-explored actions"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
