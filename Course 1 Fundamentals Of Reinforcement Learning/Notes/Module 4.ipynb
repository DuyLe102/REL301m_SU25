{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81e42b8c",
   "metadata": {},
   "source": [
    "## 1. Policies: The Agent's Playbook \n",
    "\n",
    "### What is a Policy?\n",
    "- My \"decision-making rulebook\" - tells me what to do in each situation\n",
    "- **Input:** Current state (s)  \n",
    "- **Output:** Action to take (a)\n",
    "\n",
    "### Policy Types:\n",
    "| **Deterministic Policy**      | **Stochastic Policy**          |\n",
    "|-------------------------------|--------------------------------|\n",
    "| `π(s) = a`                    | `π(a∣s)=P(a∣s)`                |\n",
    "| Always choose same action     | Probability distribution      |\n",
    "| Simple but rigid              | Flexible but complex          |\n",
    "| Example: \"Always turn left\"   | Example: \"Turn left 80% of time\" |\n",
    "\n",
    "> Key insight: Policies ONLY care about current state - not history or time! ⏱\n",
    "\n",
    "## 2. Value Functions: The Crystal Ball \n",
    "\n",
    "### State-Value Function v(s)\n",
    "- \"How good is this state?\"\n",
    "- Expected total future rewards from state s:  \n",
    "  `v_π(s) = 𝔼[G_t | S_t = s]`  \n",
    "- Example: In chess, value of having queen advantage\n",
    "\n",
    "### Action-Value Function q(s,a)\n",
    "- \"How good is this action in this state?\"\n",
    "- Expected total future rewards after taking a in s:  \n",
    "  `q_π(s,a) = 𝔼[G_t | S_t = s, A_t = a]`  \n",
    "- Example: Value of moving pawn vs castle in specific position\n",
    "\n",
    "### Why We Need Them:\n",
    "1. **Don't wait for final outcome** → Predict future early\n",
    "2. **Handle randomness** → Account for stochastic environments\n",
    "3. **Compare alternatives** → Make optimal decisions\n",
    "\n",
    "## 3. Bellman Equations: The RL Secret Sauce \n",
    "\n",
    "### State-Value Bellman Eq\n",
    "`v_π(s) = ∑_a π(a|s) ∑_s' ∑_r p(s',r|s,a) [r + γv_π(s')]`\n",
    "\n",
    "**Translation:**  \n",
    "Value of state s =  \n",
    "[Avg over actions I might take] ×  \n",
    "[Avg over possible next states/rewards] ×  \n",
    "[Immediate reward + Discounted value of next state]\n",
    "\n",
    "### Action-Value Bellman Eq\n",
    "`q_π(s,a) = ∑_s' ∑_r p(s',r|s,a) [r + γ ∑_a' π(a'|s') q_π(s',a')]`\n",
    "\n",
    "**Translation:**  \n",
    "Value of (s,a) =  \n",
    "[Avg over possible outcomes] ×  \n",
    "[Immediate reward + Discounted value of next action choices]\n",
    "\n",
    "> My cheat: Both equations are just \"reward now + discounted future value\" \n",
    "\n",
    "## 4. Optimal Policies: The RL Endgame \n",
    "\n",
    "### What Makes a Policy Optimal? (π*)\n",
    "- Beats or equals ALL other policies in EVERY state\n",
    "- Has highest possible value functions:  \n",
    "  `v_*(s) = max_π v_π(s)`  \n",
    "  `q_*(s,a) = max_π q_π(s,a)`\n",
    "\n",
    "### Optimal Value Functions:\n",
    "`v_*(s) = max_a ∑_s' ∑_r p(s',r|s,a) [r + γv_*(s')]`  \n",
    "`q_*(s,a) = ∑_s' ∑_r p(s',r|s,a) [r + γ max_a' q_*(s',a')]`\n",
    "\n",
    "### Key Properties:\n",
    "1. **Universality:** Same v* and q* for all optimal policies\n",
    "2. **Greediness:** Optimal policy is greedy w.r.t q*  \n",
    "   `π*(s) = argmax_a q_*(s,a)`\n",
    "3. **Recursive Magic:** Future optimal ⇒ Current optimal\n",
    "\n",
    "## Student Cheat Sheet 📝\n",
    "\n",
    "| **Concept**          | **Formula**                      | **When to Use**                  |\n",
    "|----------------------|----------------------------------|----------------------------------|\n",
    "| State-Value          | `v_π(s) = 𝔼[G_t⎜S_t=s]`         | Evaluate positions               |\n",
    "| Action-Value         | `q_π(s,a) = 𝔼[G_t⎜S_t=s,A_t=a]` | Compare moves                    |\n",
    "| State Bellman        | `v_π(s) = ... + γv_π(s')`       | Value iteration                  |\n",
    "| Action Bellman       | `q_π(s,a) = ... + γq_π(s',a')`  | Q-learning                       |\n",
    "| Optimal Value        | `v_*(s) = max_a[...]`           | Finding best strategy            |\n",
    "\n",
    "> Bellman equations ALWAYS have that γ discount! Don't forget it! "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
