{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81e42b8c",
   "metadata": {},
   "source": [
    "## 1. Policies: The Agent's Playbook \n",
    "\n",
    "### What is a Policy?\n",
    "- My \"decision-making rulebook\" - tells me what to do in each situation\n",
    "- **Input:** Current state (s)  \n",
    "- **Output:** Action to take (a)\n",
    "\n",
    "### Policy Types:\n",
    "| **Deterministic Policy**      | **Stochastic Policy**          |\n",
    "|-------------------------------|--------------------------------|\n",
    "| `Ï€(s) = a`                    | `Ï€(aâˆ£s)=P(aâˆ£s)`                |\n",
    "| Always choose same action     | Probability distribution      |\n",
    "| Simple but rigid              | Flexible but complex          |\n",
    "| Example: \"Always turn left\"   | Example: \"Turn left 80% of time\" |\n",
    "\n",
    "> Key insight: Policies ONLY care about current state - not history or time! â±\n",
    "\n",
    "## 2. Value Functions: The Crystal Ball \n",
    "\n",
    "### State-Value Function v(s)\n",
    "- \"How good is this state?\"\n",
    "- Expected total future rewards from state s:  \n",
    "  `v_Ï€(s) = ð”¼[G_t | S_t = s]`  \n",
    "- Example: In chess, value of having queen advantage\n",
    "\n",
    "### Action-Value Function q(s,a)\n",
    "- \"How good is this action in this state?\"\n",
    "- Expected total future rewards after taking a in s:  \n",
    "  `q_Ï€(s,a) = ð”¼[G_t | S_t = s, A_t = a]`  \n",
    "- Example: Value of moving pawn vs castle in specific position\n",
    "\n",
    "### Why We Need Them:\n",
    "1. **Don't wait for final outcome** â†’ Predict future early\n",
    "2. **Handle randomness** â†’ Account for stochastic environments\n",
    "3. **Compare alternatives** â†’ Make optimal decisions\n",
    "\n",
    "## 3. Bellman Equations: The RL Secret Sauce \n",
    "\n",
    "### State-Value Bellman Eq\n",
    "`v_Ï€(s) = âˆ‘_a Ï€(a|s) âˆ‘_s' âˆ‘_r p(s',r|s,a) [r + Î³v_Ï€(s')]`\n",
    "\n",
    "**Translation:**  \n",
    "Value of state s =  \n",
    "[Avg over actions I might take] Ã—  \n",
    "[Avg over possible next states/rewards] Ã—  \n",
    "[Immediate reward + Discounted value of next state]\n",
    "\n",
    "### Action-Value Bellman Eq\n",
    "`q_Ï€(s,a) = âˆ‘_s' âˆ‘_r p(s',r|s,a) [r + Î³ âˆ‘_a' Ï€(a'|s') q_Ï€(s',a')]`\n",
    "\n",
    "**Translation:**  \n",
    "Value of (s,a) =  \n",
    "[Avg over possible outcomes] Ã—  \n",
    "[Immediate reward + Discounted value of next action choices]\n",
    "\n",
    "> My cheat: Both equations are just \"reward now + discounted future value\" \n",
    "\n",
    "## 4. Optimal Policies: The RL Endgame \n",
    "\n",
    "### What Makes a Policy Optimal? (Ï€*)\n",
    "- Beats or equals ALL other policies in EVERY state\n",
    "- Has highest possible value functions:  \n",
    "  `v_*(s) = max_Ï€ v_Ï€(s)`  \n",
    "  `q_*(s,a) = max_Ï€ q_Ï€(s,a)`\n",
    "\n",
    "### Optimal Value Functions:\n",
    "`v_*(s) = max_a âˆ‘_s' âˆ‘_r p(s',r|s,a) [r + Î³v_*(s')]`  \n",
    "`q_*(s,a) = âˆ‘_s' âˆ‘_r p(s',r|s,a) [r + Î³ max_a' q_*(s',a')]`\n",
    "\n",
    "### Key Properties:\n",
    "1. **Universality:** Same v* and q* for all optimal policies\n",
    "2. **Greediness:** Optimal policy is greedy w.r.t q*  \n",
    "   `Ï€*(s) = argmax_a q_*(s,a)`\n",
    "3. **Recursive Magic:** Future optimal â‡’ Current optimal\n",
    "\n",
    "## Student Cheat Sheet ðŸ“\n",
    "\n",
    "| **Concept**          | **Formula**                      | **When to Use**                  |\n",
    "|----------------------|----------------------------------|----------------------------------|\n",
    "| State-Value          | `v_Ï€(s) = ð”¼[G_tâŽœS_t=s]`         | Evaluate positions               |\n",
    "| Action-Value         | `q_Ï€(s,a) = ð”¼[G_tâŽœS_t=s,A_t=a]` | Compare moves                    |\n",
    "| State Bellman        | `v_Ï€(s) = ... + Î³v_Ï€(s')`       | Value iteration                  |\n",
    "| Action Bellman       | `q_Ï€(s,a) = ... + Î³q_Ï€(s',a')`  | Q-learning                       |\n",
    "| Optimal Value        | `v_*(s) = max_a[...]`           | Finding best strategy            |\n",
    "\n",
    "> Bellman equations ALWAYS have that Î³ discount! Don't forget it! "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
