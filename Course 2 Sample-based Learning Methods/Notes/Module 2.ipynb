{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Methods\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tại sao cần Monte Carlo (MC)?**  \n",
    "\n",
    "### Vấn đề với Dynamic Programming (DP):\n",
    "- **Cần exact transition probabilities** - mà thực tế không có\n",
    "  - Ví dụ: Dự báo thời tiết không thể cho exact probabilities\n",
    "- **Computationally messy** với large state spaces\n",
    "- Cần model hoàn chỉnh của environment\n",
    "\n",
    "### MC Solution:\n",
    "- Ước tính values bằng cách **average random samples**\n",
    "- Giống như tung xúc xắc nhiều lần để tính trung bình\n",
    "- **Model-free**: Chỉ cần experience, không cần biết transition probabilities\n",
    "\n",
    "**💡 Core idea**: Nếu không biết exact math → dùng sampling!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MC Prediction: Ước tính V ≈ v_π**  \n",
    "\n",
    "**Mục tiêu**: Tìm state value V dưới policy π\n",
    "\n",
    "### Algorithm:\n",
    "\n",
    "**1. Khởi tạo**:  \n",
    "- `V(s)` arbitrary cho tất cả states  \n",
    "- Empty list `Return(s)` cho mỗi state  \n",
    "\n",
    "**2. Mỗi episode**:  \n",
    "- **Generate episode**: S₀, A₀, R₁, S₁, ..., R_T using π  \n",
    "- `G ← 0` (total discounted reward)  \n",
    "- **Loop backwards** (t = T-1 to 0):  \n",
    "  - Update: `G ← γG + R_{t+1}`  \n",
    "  - Append G to `Return(S_t)`  \n",
    "  - Update `V(S_t)` = average(`Return(S_t)`)\n",
    "\n",
    "**🔑 Key insight**: Học từ **complete episodes**, không update giữa chừng như TD\n",
    "\n",
    "**⚠️ Nhược điểm**: Phải chờ episode kết thúc mới update được"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Action Values (Q-values)**  \n",
    "\n",
    "### Tại sao cần Q thay vì chỉ V?\n",
    "- **V(s)** chỉ cho biết state tốt như thế nào\n",
    "- **Q(s,a)** cho biết action nào tốt nhất ở state đó!\n",
    "- **Formula**: `q_π(s,a) = E[G_t | S_t=s, A_t=a]`\n",
    "\n",
    "### Problem:\n",
    "- **Cần estimates cho TẤT CẢ actions** để pick best one\n",
    "- Nếu chỉ follow 1 policy → chỉ thấy 1 action per state\n",
    "- **Solution**: Cần exploration!\n",
    "\n",
    "**💭 Think about it**: Làm sao biết action nào tốt nhất nếu chưa thử hết?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MC Exploring Starts**  \n",
    "\n",
    "**Mục tiêu**: Ước tính π ≈ π* (optimal policy)  \n",
    "\n",
    "### Ý tưởng:\n",
    "- **Bắt buộc** start từ mọi state-action pair có thể\n",
    "- Đảm bảo tất cả (s,a) được explore\n",
    "\n",
    "### Algorithm:\n",
    "**1. Initialize**: \n",
    "- `Q(s,a)` arbitrarily + empty `Returns(s,a)`  \n",
    "\n",
    "**2. Mỗi episode**:  \n",
    "- **Exploring Start**: Random pick S₀, A₀ (tất cả pairs đều possible)  \n",
    "- Generate episode using π  \n",
    "- Update G backwards, append to `Returns(S_t,A_t)`  \n",
    "- Set `Q(S_t,A_t)` = average(`Returns`) và `π(S_t) = argmax Q(S_t,a)`\n",
    "\n",
    "### Vấn đề:\n",
    "- ❌ **Impractical**: Không thể start từ bất kỳ state nào trong real world\n",
    "- ❌ **Unrealistic assumption**: Exploring starts không có trong thực tế\n",
    "\n",
    "**🤔 Real world**: Bạn không thể bắt đầu game chess từ bất kỳ position nào!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ε-Soft Policies**  \n",
    "\n",
    "**Giải pháp cho Exploring Starts**: Đảm bảo exploration liên tục\n",
    "\n",
    "### Định nghĩa:\n",
    "- **Rule**: Mọi action có probability ≥ ε/|A(s)|\n",
    "- **Nghĩa là**: Không action nào có probability = 0\n",
    "- **Luôn có chance** thử action \"tệ\"\n",
    "\n",
    "### Ưu điểm:\n",
    "- ✅ **Không cần \"start anywhere\"**  \n",
    "- ✅ **Luôn visit tất cả state-action pairs**  \n",
    "- ✅ **Practical** trong real applications\n",
    "\n",
    "### Nhược điểm:\n",
    "- ❌ **Policy là stochastic** (không pure optimal)\n",
    "- ❌ **Trade-off**: Exploration vs Exploitation\n",
    "\n",
    "**💡 Intuition**: Thỉnh thoảng làm điều \"stupid\" để học được điều mới!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MC Control (ε-Soft Policies)**  \n",
    "\n",
    "**Mục tiêu**: Ước tính π ≈ π* mà không cần exploring starts\n",
    "\n",
    "### Algorithm:\n",
    "\n",
    "**1. Initialize**:  \n",
    "- Arbitrary ε-soft π, Q(s,a), empty `Returns(s,a)`  \n",
    "\n",
    "**2. Mỗi episode**:  \n",
    "- Generate episode using π  \n",
    "- Update G backwards, append to `Returns(S_t,A_t)`  \n",
    "- Update `Q(S_t,A_t)` = average(`Returns`)  \n",
    "- **Update policy**:  \n",
    "  - `A* = argmax_a Q(S_t, a)` (best action)\n",
    "  - **Cho tất cả actions a ở state S_t**:\n",
    "    ```\n",
    "    π(a|S_t) = {\n",
    "      1 - ε + ε/|A(s)|    if a = A* (best action)\n",
    "      ε/|A(s)|           otherwise (other actions)\n",
    "    }\n",
    "    ```\n",
    "\n",
    "### Logic:\n",
    "- **Best action**: Có probability cao nhất\n",
    "- **Other actions**: Vẫn có chance được chọn\n",
    "- **Balance**: Exploit best + Explore others\n",
    "\n",
    "**🎯 Result**: Policy vừa greedy vừa có exploration!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Off-Policy Learning**  \n",
    "\n",
    "**Ý tưởng**: Học target policy π trong khi follow behavior policy b\n",
    "\n",
    "### Phân biệt:\n",
    "- **On-policy**: Học/evaluate **cùng** policy đang dùng để act  \n",
    "- **Off-policy**: Học/evaluate **khác** policy (π) với acting policy (b)  \n",
    "\n",
    "### Ứng dụng chính:\n",
    "- **Exploration**: b = exploratory, π = optimal\n",
    "- **Learning from demos**: b = human behavior, π = optimal policy\n",
    "- **Safety**: b = safe policy, π = aggressive optimal policy\n",
    "\n",
    "### Điều kiện bắt buộc - Coverage:\n",
    "```\n",
    "π(a|s) > 0 ⟹ b(a|s) > 0\n",
    "```\n",
    "**Nghĩa**: Nếu π muốn thử action → b phải thỉnh thoảng thử action đó!\n",
    "\n",
    "**🎪 Ví dụ**: Học lái xe từ người mới học (b) để trở thành tài xế giỏi (π)\n",
    "\n",
    "**⚠️ Critical**: Không có coverage → không học được!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Importance Sampling**  \n",
    "\n",
    "**Vấn đề**: Làm sao estimate expected return của π khi chỉ có samples từ b?\n",
    "\n",
    "### Math Trick:\n",
    "```\n",
    "E_π[X] = E_b[X · ρ(X)]\n",
    "```\n",
    "Trong đó: `ρ(X) = π(X)/b(X)` (importance sampling ratio)\n",
    "\n",
    "### Estimate từ samples:\n",
    "```\n",
    "E_π[X] ≈ (1/n) ∑ᵢ xᵢ · ρ(xᵢ)\n",
    "```\n",
    "\n",
    "### Intuition:\n",
    "- **ρ > 1**: π thích action này hơn b → **weight up**\n",
    "- **ρ < 1**: π ít thích action này hơn b → **weight down**\n",
    "- **ρ = 1**: π và b như nhau → **no adjustment**\n",
    "\n",
    "### Vấn đề:\n",
    "- **High variance**: Nếu ρ rất lớn → estimates không stable\n",
    "- **Rare events**: Nếu b hiếm khi thử action mà π thích → ít data\n",
    "\n",
    "**🎯 Bottom line**: Powerful technique nhưng cần careful với variance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ghi Chú Cá Nhân**  \n",
    "\n",
    "### **MC vs DP**:\n",
    "- **MC**: Không cần model (chỉ cần samples!), nhưng high variance  \n",
    "- **DP**: Cần model đầy đủ, nhưng low variance nếu model chính xác\n",
    "- **MC winner**: Khi environment unknown hoặc too complex\n",
    "\n",
    "### **ε-soft trade-off**:\n",
    "- **Small ε**: Gần như greedy, ít exploration\n",
    "- **Large ε**: Nhiều exploration, nhưng policy kém optimal\n",
    "- **Sweet spot**: Tùy thuộc vào problem và stage of learning\n",
    "\n",
    "### **Off-policy pro tips**:\n",
    "- Dùng b để **explore**, π để **optimize**\n",
    "- **Coverage is non-negotiable** - không có thì toang!\n",
    "- Importance sampling powerful nhưng **watch out for variance**\n",
    "\n",
    "### **Khi nào dùng MC?**\n",
    "- Environment unknown\n",
    "- Episodes có finite length\n",
    "- Cần model-free approach\n",
    "- Có thời gian để collect nhiều episodes\n",
    "\n",
    "---  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
