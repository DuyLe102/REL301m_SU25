{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Methods\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Táº¡i sao cáº§n Monte Carlo (MC)?**  \n",
    "\n",
    "### Váº¥n Ä‘á» vá»›i Dynamic Programming (DP):\n",
    "- **Cáº§n exact transition probabilities** - mÃ  thá»±c táº¿ khÃ´ng cÃ³\n",
    "  - VÃ­ dá»¥: Dá»± bÃ¡o thá»i tiáº¿t khÃ´ng thá»ƒ cho exact probabilities\n",
    "- **Computationally messy** vá»›i large state spaces\n",
    "- Cáº§n model hoÃ n chá»‰nh cá»§a environment\n",
    "\n",
    "### MC Solution:\n",
    "- Æ¯á»›c tÃ­nh values báº±ng cÃ¡ch **average random samples**\n",
    "- Giá»‘ng nhÆ° tung xÃºc xáº¯c nhiá»u láº§n Ä‘á»ƒ tÃ­nh trung bÃ¬nh\n",
    "- **Model-free**: Chá»‰ cáº§n experience, khÃ´ng cáº§n biáº¿t transition probabilities\n",
    "\n",
    "**ğŸ’¡ Core idea**: Náº¿u khÃ´ng biáº¿t exact math â†’ dÃ¹ng sampling!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MC Prediction: Æ¯á»›c tÃ­nh V â‰ˆ v_Ï€**  \n",
    "\n",
    "**Má»¥c tiÃªu**: TÃ¬m state value V dÆ°á»›i policy Ï€\n",
    "\n",
    "### Algorithm:\n",
    "\n",
    "**1. Khá»Ÿi táº¡o**:  \n",
    "- `V(s)` arbitrary cho táº¥t cáº£ states  \n",
    "- Empty list `Return(s)` cho má»—i state  \n",
    "\n",
    "**2. Má»—i episode**:  \n",
    "- **Generate episode**: Sâ‚€, Aâ‚€, Râ‚, Sâ‚, ..., R_T using Ï€  \n",
    "- `G â† 0` (total discounted reward)  \n",
    "- **Loop backwards** (t = T-1 to 0):  \n",
    "  - Update: `G â† Î³G + R_{t+1}`  \n",
    "  - Append G to `Return(S_t)`  \n",
    "  - Update `V(S_t)` = average(`Return(S_t)`)\n",
    "\n",
    "**ğŸ”‘ Key insight**: Há»c tá»« **complete episodes**, khÃ´ng update giá»¯a chá»«ng nhÆ° TD\n",
    "\n",
    "**âš ï¸ NhÆ°á»£c Ä‘iá»ƒm**: Pháº£i chá» episode káº¿t thÃºc má»›i update Ä‘Æ°á»£c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Action Values (Q-values)**  \n",
    "\n",
    "### Táº¡i sao cáº§n Q thay vÃ¬ chá»‰ V?\n",
    "- **V(s)** chá»‰ cho biáº¿t state tá»‘t nhÆ° tháº¿ nÃ o\n",
    "- **Q(s,a)** cho biáº¿t action nÃ o tá»‘t nháº¥t á»Ÿ state Ä‘Ã³!\n",
    "- **Formula**: `q_Ï€(s,a) = E[G_t | S_t=s, A_t=a]`\n",
    "\n",
    "### Problem:\n",
    "- **Cáº§n estimates cho Táº¤T Cáº¢ actions** Ä‘á»ƒ pick best one\n",
    "- Náº¿u chá»‰ follow 1 policy â†’ chá»‰ tháº¥y 1 action per state\n",
    "- **Solution**: Cáº§n exploration!\n",
    "\n",
    "**ğŸ’­ Think about it**: LÃ m sao biáº¿t action nÃ o tá»‘t nháº¥t náº¿u chÆ°a thá»­ háº¿t?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MC Exploring Starts**  \n",
    "\n",
    "**Má»¥c tiÃªu**: Æ¯á»›c tÃ­nh Ï€ â‰ˆ Ï€* (optimal policy)  \n",
    "\n",
    "### Ã tÆ°á»Ÿng:\n",
    "- **Báº¯t buá»™c** start tá»« má»i state-action pair cÃ³ thá»ƒ\n",
    "- Äáº£m báº£o táº¥t cáº£ (s,a) Ä‘Æ°á»£c explore\n",
    "\n",
    "### Algorithm:\n",
    "**1. Initialize**: \n",
    "- `Q(s,a)` arbitrarily + empty `Returns(s,a)`  \n",
    "\n",
    "**2. Má»—i episode**:  \n",
    "- **Exploring Start**: Random pick Sâ‚€, Aâ‚€ (táº¥t cáº£ pairs Ä‘á»u possible)  \n",
    "- Generate episode using Ï€  \n",
    "- Update G backwards, append to `Returns(S_t,A_t)`  \n",
    "- Set `Q(S_t,A_t)` = average(`Returns`) vÃ  `Ï€(S_t) = argmax Q(S_t,a)`\n",
    "\n",
    "### Váº¥n Ä‘á»:\n",
    "- âŒ **Impractical**: KhÃ´ng thá»ƒ start tá»« báº¥t ká»³ state nÃ o trong real world\n",
    "- âŒ **Unrealistic assumption**: Exploring starts khÃ´ng cÃ³ trong thá»±c táº¿\n",
    "\n",
    "**ğŸ¤” Real world**: Báº¡n khÃ´ng thá»ƒ báº¯t Ä‘áº§u game chess tá»« báº¥t ká»³ position nÃ o!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Îµ-Soft Policies**  \n",
    "\n",
    "**Giáº£i phÃ¡p cho Exploring Starts**: Äáº£m báº£o exploration liÃªn tá»¥c\n",
    "\n",
    "### Äá»‹nh nghÄ©a:\n",
    "- **Rule**: Má»i action cÃ³ probability â‰¥ Îµ/|A(s)|\n",
    "- **NghÄ©a lÃ **: KhÃ´ng action nÃ o cÃ³ probability = 0\n",
    "- **LuÃ´n cÃ³ chance** thá»­ action \"tá»‡\"\n",
    "\n",
    "### Æ¯u Ä‘iá»ƒm:\n",
    "- âœ… **KhÃ´ng cáº§n \"start anywhere\"**  \n",
    "- âœ… **LuÃ´n visit táº¥t cáº£ state-action pairs**  \n",
    "- âœ… **Practical** trong real applications\n",
    "\n",
    "### NhÆ°á»£c Ä‘iá»ƒm:\n",
    "- âŒ **Policy lÃ  stochastic** (khÃ´ng pure optimal)\n",
    "- âŒ **Trade-off**: Exploration vs Exploitation\n",
    "\n",
    "**ğŸ’¡ Intuition**: Thá»‰nh thoáº£ng lÃ m Ä‘iá»u \"stupid\" Ä‘á»ƒ há»c Ä‘Æ°á»£c Ä‘iá»u má»›i!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MC Control (Îµ-Soft Policies)**  \n",
    "\n",
    "**Má»¥c tiÃªu**: Æ¯á»›c tÃ­nh Ï€ â‰ˆ Ï€* mÃ  khÃ´ng cáº§n exploring starts\n",
    "\n",
    "### Algorithm:\n",
    "\n",
    "**1. Initialize**:  \n",
    "- Arbitrary Îµ-soft Ï€, Q(s,a), empty `Returns(s,a)`  \n",
    "\n",
    "**2. Má»—i episode**:  \n",
    "- Generate episode using Ï€  \n",
    "- Update G backwards, append to `Returns(S_t,A_t)`  \n",
    "- Update `Q(S_t,A_t)` = average(`Returns`)  \n",
    "- **Update policy**:  \n",
    "  - `A* = argmax_a Q(S_t, a)` (best action)\n",
    "  - **Cho táº¥t cáº£ actions a á»Ÿ state S_t**:\n",
    "    ```\n",
    "    Ï€(a|S_t) = {\n",
    "      1 - Îµ + Îµ/|A(s)|    if a = A* (best action)\n",
    "      Îµ/|A(s)|           otherwise (other actions)\n",
    "    }\n",
    "    ```\n",
    "\n",
    "### Logic:\n",
    "- **Best action**: CÃ³ probability cao nháº¥t\n",
    "- **Other actions**: Váº«n cÃ³ chance Ä‘Æ°á»£c chá»n\n",
    "- **Balance**: Exploit best + Explore others\n",
    "\n",
    "**ğŸ¯ Result**: Policy vá»«a greedy vá»«a cÃ³ exploration!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Off-Policy Learning**  \n",
    "\n",
    "**Ã tÆ°á»Ÿng**: Há»c target policy Ï€ trong khi follow behavior policy b\n",
    "\n",
    "### PhÃ¢n biá»‡t:\n",
    "- **On-policy**: Há»c/evaluate **cÃ¹ng** policy Ä‘ang dÃ¹ng Ä‘á»ƒ act  \n",
    "- **Off-policy**: Há»c/evaluate **khÃ¡c** policy (Ï€) vá»›i acting policy (b)  \n",
    "\n",
    "### á»¨ng dá»¥ng chÃ­nh:\n",
    "- **Exploration**: b = exploratory, Ï€ = optimal\n",
    "- **Learning from demos**: b = human behavior, Ï€ = optimal policy\n",
    "- **Safety**: b = safe policy, Ï€ = aggressive optimal policy\n",
    "\n",
    "### Äiá»u kiá»‡n báº¯t buá»™c - Coverage:\n",
    "```\n",
    "Ï€(a|s) > 0 âŸ¹ b(a|s) > 0\n",
    "```\n",
    "**NghÄ©a**: Náº¿u Ï€ muá»‘n thá»­ action â†’ b pháº£i thá»‰nh thoáº£ng thá»­ action Ä‘Ã³!\n",
    "\n",
    "**ğŸª VÃ­ dá»¥**: Há»c lÃ¡i xe tá»« ngÆ°á»i má»›i há»c (b) Ä‘á»ƒ trá»Ÿ thÃ nh tÃ i xáº¿ giá»i (Ï€)\n",
    "\n",
    "**âš ï¸ Critical**: KhÃ´ng cÃ³ coverage â†’ khÃ´ng há»c Ä‘Æ°á»£c!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Importance Sampling**  \n",
    "\n",
    "**Váº¥n Ä‘á»**: LÃ m sao estimate expected return cá»§a Ï€ khi chá»‰ cÃ³ samples tá»« b?\n",
    "\n",
    "### Math Trick:\n",
    "```\n",
    "E_Ï€[X] = E_b[X Â· Ï(X)]\n",
    "```\n",
    "Trong Ä‘Ã³: `Ï(X) = Ï€(X)/b(X)` (importance sampling ratio)\n",
    "\n",
    "### Estimate tá»« samples:\n",
    "```\n",
    "E_Ï€[X] â‰ˆ (1/n) âˆ‘áµ¢ xáµ¢ Â· Ï(xáµ¢)\n",
    "```\n",
    "\n",
    "### Intuition:\n",
    "- **Ï > 1**: Ï€ thÃ­ch action nÃ y hÆ¡n b â†’ **weight up**\n",
    "- **Ï < 1**: Ï€ Ã­t thÃ­ch action nÃ y hÆ¡n b â†’ **weight down**\n",
    "- **Ï = 1**: Ï€ vÃ  b nhÆ° nhau â†’ **no adjustment**\n",
    "\n",
    "### Váº¥n Ä‘á»:\n",
    "- **High variance**: Náº¿u Ï ráº¥t lá»›n â†’ estimates khÃ´ng stable\n",
    "- **Rare events**: Náº¿u b hiáº¿m khi thá»­ action mÃ  Ï€ thÃ­ch â†’ Ã­t data\n",
    "\n",
    "**ğŸ¯ Bottom line**: Powerful technique nhÆ°ng cáº§n careful vá»›i variance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ghi ChÃº CÃ¡ NhÃ¢n**  \n",
    "\n",
    "### **MC vs DP**:\n",
    "- **MC**: KhÃ´ng cáº§n model (chá»‰ cáº§n samples!), nhÆ°ng high variance  \n",
    "- **DP**: Cáº§n model Ä‘áº§y Ä‘á»§, nhÆ°ng low variance náº¿u model chÃ­nh xÃ¡c\n",
    "- **MC winner**: Khi environment unknown hoáº·c too complex\n",
    "\n",
    "### **Îµ-soft trade-off**:\n",
    "- **Small Îµ**: Gáº§n nhÆ° greedy, Ã­t exploration\n",
    "- **Large Îµ**: Nhiá»u exploration, nhÆ°ng policy kÃ©m optimal\n",
    "- **Sweet spot**: TÃ¹y thuá»™c vÃ o problem vÃ  stage of learning\n",
    "\n",
    "### **Off-policy pro tips**:\n",
    "- DÃ¹ng b Ä‘á»ƒ **explore**, Ï€ Ä‘á»ƒ **optimize**\n",
    "- **Coverage is non-negotiable** - khÃ´ng cÃ³ thÃ¬ toang!\n",
    "- Importance sampling powerful nhÆ°ng **watch out for variance**\n",
    "\n",
    "### **Khi nÃ o dÃ¹ng MC?**\n",
    "- Environment unknown\n",
    "- Episodes cÃ³ finite length\n",
    "- Cáº§n model-free approach\n",
    "- CÃ³ thá»i gian Ä‘á»ƒ collect nhiá»u episodes\n",
    "\n",
    "---  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
