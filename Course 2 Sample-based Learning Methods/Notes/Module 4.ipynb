{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA & Q-Learning\n",
    "\n",
    "## SARSA Algorithm\n",
    "\n",
    "### √ù nghƒ©a t√™n g·ªçi\n",
    "**SARSA** = **S**tate **A**ction **R**eward **S**tate **A**ction\n",
    "\n",
    "```\n",
    "S ‚Üí A ‚Üí R ‚Üí S ‚Üí A\n",
    "‚Üì   ‚Üì   ‚Üì   ‚Üì   ‚Üì\n",
    "S‚Çú  A‚Çú  R‚Çú‚Çä‚ÇÅ S‚Çú‚Çä‚ÇÅ A‚Çú‚Çä‚ÇÅ\n",
    "```\n",
    "\n",
    "### C√°ch ho·∫°t ƒë·ªông c·ªßa SARSA\n",
    "\n",
    "SARSA d·ª± ƒëo√°n **Q-values** cho c√°c c·∫∑p state-action (kh√°c v·ªõi TD learning ch·ªâ estimate V-values)\n",
    "\n",
    "**Quy tr√¨nh 3 b∆∞·ªõc**:\n",
    "1. **Ch·ªçn action**: Agent ch·ªçn action A‚Çú ·ªü state S‚Çú ‚Üí t·∫°o c·∫∑p (S‚Çú, A‚Çú)\n",
    "2. **Quan s√°t**: Th·ª±c hi·ªán action, nh·∫≠n reward R‚Çú‚Çä‚ÇÅ v√† next state S‚Çú‚Çä‚ÇÅ  \n",
    "3. **Commit next action**: Agent **ch·ªçn tr∆∞·ªõc** A‚Çú‚Çä‚ÇÅ r·ªìi m·ªõi update Q-value\n",
    "\n",
    "**üîë Key insight**: SARSA update m·ªói step thay v√¨ ch·ªù cu·ªëi episode!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA Update Equation\n",
    "\n",
    "```\n",
    "Q(S‚Çú, A‚Çú) ‚Üê Q(S‚Çú, A‚Çú) + Œ±[R‚Çú‚Çä‚ÇÅ + Œ≥Q(S‚Çú‚Çä‚ÇÅ, A‚Çú‚Çä‚ÇÅ) - Q(S‚Çú, A‚Çú)]\n",
    "```\n",
    "\n",
    "**Breakdown**:\n",
    "- `Q(S‚Çú, A‚Çú)`: Current Q-value estimate\n",
    "- `Œ±`: Learning rate\n",
    "- `R‚Çú‚Çä‚ÇÅ + Œ≥Q(S‚Çú‚Çä‚ÇÅ, A‚Çú‚Çä‚ÇÅ)`: Target value (bootstrap t·ª´ next Q-value)\n",
    "- `[...]`: TD error\n",
    "\n",
    "**‚ö†Ô∏è ƒêi·ªÉm quan tr·ªçng**: SARSA d√πng action **ƒë√£ ƒë∆∞·ª£c ch·ªçn** A‚Çú‚Çä‚ÇÅ ƒë·ªÉ update ‚Üí **on-policy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning vs SARSA\n",
    "\n",
    "### S·ª± kh√°c bi·ªát c∆° b·∫£n\n",
    "\n",
    "| Aspect | SARSA | Q-Learning |\n",
    "|--------|-------|------------|\n",
    "| **Base on** | Bellman Equation | Bellman **Optimality** Equation |\n",
    "| **Policy type** | On-policy | Off-policy |\n",
    "| **Action for update** | Actual next action A‚Çú‚Çä‚ÇÅ | **Max** over all actions |\n",
    "| **Update formula** | `Q(s,a) + Œ±[r + Œ≥Q(s',a') - Q(s,a)]` | `Q(s,a) + Œ±[r + Œ≥max Q(s',a) - Q(s,a)]` |\n",
    "\n",
    "### Q-Learning Algorithm\n",
    "\n",
    "**Input**: \n",
    "- Learning rate Œ± ‚àà (0,1]\n",
    "- Exploration Œµ > 0\n",
    "\n",
    "**Steps**:\n",
    "1. **Init**: Q(s,a) arbitrarily, Q(terminal,¬∑) = 0\n",
    "2. **M·ªói episode**:\n",
    "   - Initialize state S\n",
    "   - **M·ªói step**:\n",
    "     - Ch·ªçn A t·ª´ S d√πng policy t·ª´ Q (vd: Œµ-greedy)\n",
    "     - Th·ª±c hi·ªán A, quan s√°t R, S'\n",
    "     - **Update**: `Q(S,A) ‚Üê Q(S,A) + Œ±[R + Œ≥max Q(S',a) - Q(S,A)]`\n",
    "     - S ‚Üê S'\n",
    "   - ƒê·∫øn khi S terminal\n",
    "\n",
    "**üî• ƒêi·ªÉm m·∫°nh Q-learning**: Kh√¥ng c·∫ßn bi·∫øt policy ƒëang follow, lu√¥n h·ªçc optimal policy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected SARSA \n",
    "\n",
    "### √ù t∆∞·ªüng\n",
    "Thay v√¨ sample 1 action nh∆∞ SARSA th√¥ng th∆∞·ªùng, **Expected SARSA** t√≠nh **expectation** tr√™n t·∫•t c·∫£ possible actions theo policy hi·ªán t·∫°i.\n",
    "\n",
    "### Update Formula\n",
    "```\n",
    "Q(S‚Çú, A‚Çú) ‚Üê Q(S‚Çú, A‚Çú) + Œ±[R‚Çú‚Çä‚ÇÅ + Œ≥‚àëœÄ(a'|S‚Çú‚Çä‚ÇÅ)Q(S‚Çú‚Çä‚ÇÅ, a') - Q(S‚Çú, A‚Çú)]\n",
    "\n",
    "```\n",
    "\n",
    "**So v·ªõi SARSA th√¥ng th∆∞·ªùng**:\n",
    "- SARSA: D√πng Q(S‚Çú‚Çä‚ÇÅ, A‚Çú‚Çä‚ÇÅ) - **1 action c·ª• th·ªÉ**\n",
    "- Expected SARSA: D√πng ‚àëœÄ(a'|S‚Çú‚Çä‚ÇÅ)Q(S‚Çú‚Çä‚ÇÅ, a') - **weighted average t·∫•t c·∫£ actions**\n",
    "\n",
    "### Trade-offs\n",
    "\n",
    "**∆Øu ƒëi·ªÉm**:\n",
    "- ‚úÖ **Lower variance** (v√¨ d√πng expectation thay v√¨ sample)\n",
    "- ‚úÖ **H·ªçc nhanh h∆°n** policy t·ªët\n",
    "- ‚úÖ **Robust** v·ªõi large step sizes\n",
    "\n",
    "**Nh∆∞·ª£c ƒëi·ªÉm**:\n",
    "- ‚ùå **Expensive** h∆°n (ph·∫£i t√≠nh expectation)\n",
    "- ‚ùå C·∫ßn bi·∫øt policy œÄ(a'|s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T·ªïng k·∫øt & So s√°nh\n",
    "\n",
    "### Khi n√†o d√πng g√¨?\n",
    "\n",
    "**SARSA (On-policy)**:\n",
    "- Khi mu·ªën agent h·ªçc policy m√† n√≥ ƒëang follow\n",
    "- Safer trong real-world applications\n",
    "- T·ªët cho exploration-exploitation balance\n",
    "\n",
    "**Q-Learning (Off-policy)**:\n",
    "- Khi mu·ªën h·ªçc optimal policy b·∫•t k·ªÉ ƒëang follow policy g√¨\n",
    "- Flexible h∆°n, c√≥ th·ªÉ d√πng data t·ª´ b·∫•t k·ª≥ policy n√†o\n",
    "- Converge ƒë·∫øn optimal Q* n·∫øu ƒëi·ªÅu ki·ªán th·ªèa m√£n\n",
    "\n",
    "**Expected SARSA**:\n",
    "- Khi computational cost kh√¥ng l√† v·∫•n ƒë·ªÅ\n",
    "- Mu·ªën stable learning v·ªõi lower variance\n",
    "- C√≥ th·ªÉ ho·∫°t ƒë·ªông nh∆∞ on-policy ho·∫∑c off-policy\n",
    "\n",
    "---\n",
    "\n",
    "*Personal note: Q-learning th∆∞·ªùng ƒë∆∞·ª£c d√πng nhi·ªÅu h∆°n trong practice v√¨ t√≠nh flexibility, nh∆∞ng SARSA c√≥ th·ªÉ an to√†n h∆°n trong nh·ªØng t√¨nh hu·ªëng real-world. Expected SARSA l√† middle ground t·ªët n·∫øu computational budget cho ph√©p.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
