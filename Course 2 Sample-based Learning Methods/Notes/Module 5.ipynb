{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dyna-Q & Model-Based Planning\n",
    "\n",
    "## Model lÃ  gÃ¬? \n",
    "\n",
    "**Model** = CÃ¡ch lÆ°u trá»¯ kiáº¿n thá»©c vá» **transition dynamics** vÃ  **reward dynamics** cá»§a environment\n",
    "\n",
    "### Táº¡i sao cáº§n model?\n",
    "- DÃ¹ng cho **planning** - quÃ¡ trÃ¬nh sá»­ dá»¥ng model Ä‘á»ƒ cáº£i thiá»‡n policy\n",
    "- CÃ¡ch planning: DÃ¹ng **Simulated Experience** + Value function updates\n",
    "- **Logic**: Estimate value tá»‘t hÆ¡n â†’ decisions tá»‘t hÆ¡n\n",
    "\n",
    "**ğŸ’¡ Ã tÆ°á»Ÿng chÃ­nh**: Thay vÃ¬ chá»‰ há»c tá»« real experience, ta cÃ³ thá»ƒ \"tÆ°á»Ÿng tÆ°á»£ng\" experience tá»« model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Loáº¡i Model \n",
    "\n",
    "### 1. Sample Models\n",
    "- **Chá»©c nÄƒng**: Táº¡o ra **1 káº¿t quáº£ cá»¥ thá»ƒ** tá»« underlying probabilities\n",
    "- **Æ¯u Ä‘iá»ƒm**: \n",
    "  - âœ… **Computationally cheap** \n",
    "  - âœ… Dá»… implement (chá»‰ cáº§n random theo rules)\n",
    "- **VÃ­ dá»¥**: Throw a dice â†’ ra sá»‘ 4 (khÃ´ng cáº§n biáº¿t P(1)=1/6, P(2)=1/6,...)\n",
    "\n",
    "### 2. Distribution Models  \n",
    "- **Chá»©c nÄƒng**: Specify **probability cá»§a Táº¤T Cáº¢ outcomes**\n",
    "- **Æ¯u Ä‘iá»ƒm**: \n",
    "  - âœ… **Chá»©a nhiá»u thÃ´ng tin hÆ¡n**\n",
    "  - âœ… Complete knowledge vá» environment\n",
    "- **NhÆ°á»£c Ä‘iá»ƒm**:\n",
    "  - âŒ **KhÃ³ specify** (cáº§n biáº¿t táº¥t cáº£ probabilities)\n",
    "  - âŒ **Very large** (exponential vá»›i sá»‘ states/actions)\n",
    "\n",
    "**ğŸ¯ Trong practice**: Sample models Ä‘Æ°á»£c dÃ¹ng nhiá»u hÆ¡n vÃ¬ practical!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Sample One-Step Q-Learning \n",
    "\n",
    "### Setup\n",
    "**Giáº£ Ä‘á»‹nh**:\n",
    "- CÃ³ sample model cá»§a transition dynamics\n",
    "- CÃ³ strategy Ä‘á»ƒ sample relevant state-action pairs\n",
    "\n",
    "### Algorithm (3 steps)\n",
    "\n",
    "1. **Random sampling**: \n",
    "   - Chá»n random (state, action) pair tá»« táº¥t cáº£ possible pairs\n",
    "   - Query sample model â†’ get next state & reward\n",
    "\n",
    "2. **Q-learning update**: \n",
    "   - Perform Q-learning update trÃªn model transition nÃ y\n",
    "   - `Q(s,a) â† Q(s,a) + Î±[r + Î³max Q(s',a') - Q(s,a)]`\n",
    "\n",
    "3. **Policy improvement**: \n",
    "   - Improve policy báº±ng cÃ¡ch **greedify** theo updated Q-values\n",
    "\n",
    "**ğŸ”‘ Key point**: Há»c tá»« **simulated experience** thay vÃ¬ chá»‰ real experience!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dyna Architecture \n",
    "\n",
    "**Ã tÆ°á»Ÿng**: Káº¿t há»£p **learning** (tá»« real experience) + **planning** (tá»« simulated experience)\n",
    "\n",
    "### Tabular Dyna-Q Algorithm\n",
    "\n",
    "**Initialization**:\n",
    "- `Q(s,a)` vÃ  `Model(s,a)` cho táº¥t cáº£ states & actions\n",
    "\n",
    "**Main Loop (forever)**:\n",
    "\n",
    "**ğŸ”´ DIRECT RL (há»c tá»« real experience)**:\n",
    "- (a) `S â† current state` (non-terminal)\n",
    "- (b) `A â† Îµ-greedy(S, Q)` \n",
    "- (c) Take action A; observe R, S'\n",
    "- (d) **Q-update**: `Q(S,A) â† Q(S,A) + Î±[R + Î³max Q(S',a) - Q(S,A)]`\n",
    "- (e) **Model update**: `Model(S,A) â† R, S'` (assuming deterministic env)\n",
    "\n",
    "**ğŸ”µ PLANNING (há»c tá»« simulated experience)**:\n",
    "- (f) **Repeat n times**:\n",
    "  - `S â† random previously observed state`\n",
    "  - `A â† random action previously taken in S`  \n",
    "  - `R, S' â† Model(S,A)` (simulate experience)\n",
    "  - **Q-update**: `Q(S,A) â† Q(S,A) + Î±[R + Î³max Q(S',a) - Q(S,A)]`\n",
    "\n",
    "**ğŸ’¡ Äiá»ƒm hay**: Má»—i real step â†’ n planning steps! TÄƒng tá»‘c learning Ä‘Ã¡ng ká»ƒ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Váº¥n Ä‘á»: Model KhÃ´ng ChÃ­nh XÃ¡c\n",
    "\n",
    "### Khi nÃ o model inaccurate?\n",
    "\n",
    "**1. Incomplete Model**:\n",
    "- **NguyÃªn nhÃ¢n**: Agent chÆ°a thá»­ háº§u háº¿t actions á»Ÿ háº§u háº¿t states\n",
    "- **Há»‡ quáº£**: Transitions cho nhá»¯ng (s,a) chÆ°a thá»­ = **missing**\n",
    "- **Timeline**: ThÆ°á»ng xáº£y ra Ä‘áº§u quÃ¡ trÃ¬nh learning\n",
    "\n",
    "**2. Changing Environment**:\n",
    "- **NguyÃªn nhÃ¢n**: Environment thay Ä‘á»•i theo thá»i gian\n",
    "- **Há»‡ quáº£**: Same action á»Ÿ same state â†’ different outcome so vá»›i trÆ°á»›c\n",
    "- **VÃ­ dá»¥**: ÄÆ°á»ng bá»‹ blocked, rules thay Ä‘á»•i, etc.\n",
    "\n",
    "### Há»‡ quáº£ cá»§a Inaccurate Model\n",
    "\n",
    "âš ï¸ **Warning**: Planning vá»›i inaccurate model chá»‰ improve policy **Ä‘á»‘i vá»›i model**, **KHÃ”NG pháº£i** environment tháº­t!\n",
    "\n",
    "â†’ Agent cÃ³ thá»ƒ há»c policy tá»‘t cho \"tháº¿ giá»›i áº£o\" nhÆ°ng tá»‡ cho tháº¿ giá»›i tháº­t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions cho Model Problems \n",
    "\n",
    "### 1. Incomplete Model\n",
    "**Dyna-Q approach**: Chá»‰ sample nhá»¯ng state-action pairs **Ä‘Ã£ Ä‘Æ°á»£c visit trÆ°á»›c Ä‘Ã³**\n",
    "- âœ… **Safe**: KhÃ´ng plan vá»›i data khÃ´ng cÃ³\n",
    "- âœ… **Practical**: DÃ¹ng experience cÃ³ sáºµn efficiently\n",
    "- âŒ **Conservative**: CÃ³ thá»ƒ miss nhá»¯ng opportunities má»›i\n",
    "\n",
    "### 2. Changing Environment: Dyna-Q+ \n",
    "\n",
    "**Ã tÆ°á»Ÿng**: Encourage agent **revisit** states periodically Ä‘á»ƒ detect changes\n",
    "\n",
    "**CÃ¡ch lÃ m**: ThÃªm **bonus reward** vÃ o planning:\n",
    "\n",
    "```\n",
    "New reward = r + ÎºâˆšÏ„\n",
    "```\n",
    "\n",
    "**Trong Ä‘Ã³**:\n",
    "- `r`: actual reward\n",
    "- `Îº`: small constant (control influence)\n",
    "- `Ï„`: time steps since transition was last tried\n",
    "\n",
    "**Logic**: \n",
    "- Transition cÃ ng lÃ¢u khÃ´ng thá»­ â†’ Ï„ cÃ ng lá»›n â†’ bonus cÃ ng lá»›n\n",
    "- Agent Ä‘Æ°á»£c khuyáº¿n khÃ­ch explore nhá»¯ng chá»— Ä‘Ã£ lÃ¢u khÃ´ng Ä‘áº¿n\n",
    "- Náº¿u environment thay Ä‘á»•i â†’ sáº½ Ä‘Æ°á»£c detect sá»›m hÆ¡n\n",
    "\n",
    "**ğŸ¯ Káº¿t quáº£**: Dyna-Q+ = Dyna-Q + exploration bonus for planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tá»•ng Káº¿t & Insights \n",
    "\n",
    "### Æ¯u Ä‘iá»ƒm cá»§a Model-Based Methods\n",
    "- âœ… **Sample efficiency**: Há»c tá»« cáº£ real + simulated experience\n",
    "- âœ… **Faster learning**: n planning steps per real step\n",
    "- âœ… **Knowledge reuse**: Model cÃ³ thá»ƒ dÃ¹ng cho nhiá»u tasks\n",
    "\n",
    "### Challenges\n",
    "- âŒ **Model accuracy**: Inaccurate model â†’ poor performance\n",
    "- âŒ **Computational cost**: Maintain & query model\n",
    "- âŒ **Environment changes**: Cáº§n detect & adapt\n",
    "\n",
    "### Khi nÃ o dÃ¹ng Model-Based?\n",
    "- **Environment stable**: Ãt thay Ä‘á»•i, model reliable\n",
    "- **Sample expensive**: Real experience khÃ³/Ä‘áº¯t Ä‘á»ƒ cÃ³\n",
    "- **Planning horizon**: Cáº§n think ahead nhiá»u steps\n",
    "\n",
    "### Khi nÃ o dÃ¹ng Model-Free?\n",
    "- **Environment dynamic**: Thay Ä‘á»•i thÆ°á»ng xuyÃªn\n",
    "- **Simple tasks**: KhÃ´ng cáº§n complex planning\n",
    "- **Real-time**: Cáº§n respond nhanh, khÃ´ng cÃ³ time Ä‘á»ƒ plan\n",
    "\n",
    "---\n",
    "\n",
    "*Personal note: Dyna-Q lÃ  má»™t breakthrough vÃ¬ combine Ä‘Æ°á»£c benefits cá»§a cáº£ model-based vÃ  model-free. Trong practice, nhiá»u state-of-the-art algorithms Ä‘á»u cÃ³ elements cá»§a cáº£ hai approaches. Key lÃ  biáº¿t khi nÃ o trust model vÃ  khi nÃ o khÃ´ng!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
