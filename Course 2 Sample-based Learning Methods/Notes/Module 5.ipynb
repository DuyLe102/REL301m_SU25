{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dyna-Q & Model-Based Planning\n",
    "\n",
    "## Model là gì? \n",
    "\n",
    "**Model** = Cách lưu trữ kiến thức về **transition dynamics** và **reward dynamics** của environment\n",
    "\n",
    "### Tại sao cần model?\n",
    "- Dùng cho **planning** - quá trình sử dụng model để cải thiện policy\n",
    "- Cách planning: Dùng **Simulated Experience** + Value function updates\n",
    "- **Logic**: Estimate value tốt hơn → decisions tốt hơn\n",
    "\n",
    "**💡 Ý tưởng chính**: Thay vì chỉ học từ real experience, ta có thể \"tưởng tượng\" experience từ model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Loại Model \n",
    "\n",
    "### 1. Sample Models\n",
    "- **Chức năng**: Tạo ra **1 kết quả cụ thể** từ underlying probabilities\n",
    "- **Ưu điểm**: \n",
    "  - ✅ **Computationally cheap** \n",
    "  - ✅ Dễ implement (chỉ cần random theo rules)\n",
    "- **Ví dụ**: Throw a dice → ra số 4 (không cần biết P(1)=1/6, P(2)=1/6,...)\n",
    "\n",
    "### 2. Distribution Models  \n",
    "- **Chức năng**: Specify **probability của TẤT CẢ outcomes**\n",
    "- **Ưu điểm**: \n",
    "  - ✅ **Chứa nhiều thông tin hơn**\n",
    "  - ✅ Complete knowledge về environment\n",
    "- **Nhược điểm**:\n",
    "  - ❌ **Khó specify** (cần biết tất cả probabilities)\n",
    "  - ❌ **Very large** (exponential với số states/actions)\n",
    "\n",
    "**🎯 Trong practice**: Sample models được dùng nhiều hơn vì practical!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Sample One-Step Q-Learning \n",
    "\n",
    "### Setup\n",
    "**Giả định**:\n",
    "- Có sample model của transition dynamics\n",
    "- Có strategy để sample relevant state-action pairs\n",
    "\n",
    "### Algorithm (3 steps)\n",
    "\n",
    "1. **Random sampling**: \n",
    "   - Chọn random (state, action) pair từ tất cả possible pairs\n",
    "   - Query sample model → get next state & reward\n",
    "\n",
    "2. **Q-learning update**: \n",
    "   - Perform Q-learning update trên model transition này\n",
    "   - `Q(s,a) ← Q(s,a) + α[r + γmax Q(s',a') - Q(s,a)]`\n",
    "\n",
    "3. **Policy improvement**: \n",
    "   - Improve policy bằng cách **greedify** theo updated Q-values\n",
    "\n",
    "**🔑 Key point**: Học từ **simulated experience** thay vì chỉ real experience!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dyna Architecture \n",
    "\n",
    "**Ý tưởng**: Kết hợp **learning** (từ real experience) + **planning** (từ simulated experience)\n",
    "\n",
    "### Tabular Dyna-Q Algorithm\n",
    "\n",
    "**Initialization**:\n",
    "- `Q(s,a)` và `Model(s,a)` cho tất cả states & actions\n",
    "\n",
    "**Main Loop (forever)**:\n",
    "\n",
    "**🔴 DIRECT RL (học từ real experience)**:\n",
    "- (a) `S ← current state` (non-terminal)\n",
    "- (b) `A ← ε-greedy(S, Q)` \n",
    "- (c) Take action A; observe R, S'\n",
    "- (d) **Q-update**: `Q(S,A) ← Q(S,A) + α[R + γmax Q(S',a) - Q(S,A)]`\n",
    "- (e) **Model update**: `Model(S,A) ← R, S'` (assuming deterministic env)\n",
    "\n",
    "**🔵 PLANNING (học từ simulated experience)**:\n",
    "- (f) **Repeat n times**:\n",
    "  - `S ← random previously observed state`\n",
    "  - `A ← random action previously taken in S`  \n",
    "  - `R, S' ← Model(S,A)` (simulate experience)\n",
    "  - **Q-update**: `Q(S,A) ← Q(S,A) + α[R + γmax Q(S',a) - Q(S,A)]`\n",
    "\n",
    "**💡 Điểm hay**: Mỗi real step → n planning steps! Tăng tốc learning đáng kể."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vấn đề: Model Không Chính Xác\n",
    "\n",
    "### Khi nào model inaccurate?\n",
    "\n",
    "**1. Incomplete Model**:\n",
    "- **Nguyên nhân**: Agent chưa thử hầu hết actions ở hầu hết states\n",
    "- **Hệ quả**: Transitions cho những (s,a) chưa thử = **missing**\n",
    "- **Timeline**: Thường xảy ra đầu quá trình learning\n",
    "\n",
    "**2. Changing Environment**:\n",
    "- **Nguyên nhân**: Environment thay đổi theo thời gian\n",
    "- **Hệ quả**: Same action ở same state → different outcome so với trước\n",
    "- **Ví dụ**: Đường bị blocked, rules thay đổi, etc.\n",
    "\n",
    "### Hệ quả của Inaccurate Model\n",
    "\n",
    "⚠️ **Warning**: Planning với inaccurate model chỉ improve policy **đối với model**, **KHÔNG phải** environment thật!\n",
    "\n",
    "→ Agent có thể học policy tốt cho \"thế giới ảo\" nhưng tệ cho thế giới thật"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions cho Model Problems \n",
    "\n",
    "### 1. Incomplete Model\n",
    "**Dyna-Q approach**: Chỉ sample những state-action pairs **đã được visit trước đó**\n",
    "- ✅ **Safe**: Không plan với data không có\n",
    "- ✅ **Practical**: Dùng experience có sẵn efficiently\n",
    "- ❌ **Conservative**: Có thể miss những opportunities mới\n",
    "\n",
    "### 2. Changing Environment: Dyna-Q+ \n",
    "\n",
    "**Ý tưởng**: Encourage agent **revisit** states periodically để detect changes\n",
    "\n",
    "**Cách làm**: Thêm **bonus reward** vào planning:\n",
    "\n",
    "```\n",
    "New reward = r + κ√τ\n",
    "```\n",
    "\n",
    "**Trong đó**:\n",
    "- `r`: actual reward\n",
    "- `κ`: small constant (control influence)\n",
    "- `τ`: time steps since transition was last tried\n",
    "\n",
    "**Logic**: \n",
    "- Transition càng lâu không thử → τ càng lớn → bonus càng lớn\n",
    "- Agent được khuyến khích explore những chỗ đã lâu không đến\n",
    "- Nếu environment thay đổi → sẽ được detect sớm hơn\n",
    "\n",
    "**🎯 Kết quả**: Dyna-Q+ = Dyna-Q + exploration bonus for planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tổng Kết & Insights \n",
    "\n",
    "### Ưu điểm của Model-Based Methods\n",
    "- ✅ **Sample efficiency**: Học từ cả real + simulated experience\n",
    "- ✅ **Faster learning**: n planning steps per real step\n",
    "- ✅ **Knowledge reuse**: Model có thể dùng cho nhiều tasks\n",
    "\n",
    "### Challenges\n",
    "- ❌ **Model accuracy**: Inaccurate model → poor performance\n",
    "- ❌ **Computational cost**: Maintain & query model\n",
    "- ❌ **Environment changes**: Cần detect & adapt\n",
    "\n",
    "### Khi nào dùng Model-Based?\n",
    "- **Environment stable**: Ít thay đổi, model reliable\n",
    "- **Sample expensive**: Real experience khó/đắt để có\n",
    "- **Planning horizon**: Cần think ahead nhiều steps\n",
    "\n",
    "### Khi nào dùng Model-Free?\n",
    "- **Environment dynamic**: Thay đổi thường xuyên\n",
    "- **Simple tasks**: Không cần complex planning\n",
    "- **Real-time**: Cần respond nhanh, không có time để plan\n",
    "\n",
    "---\n",
    "\n",
    "*Personal note: Dyna-Q là một breakthrough vì combine được benefits của cả model-based và model-free. Trong practice, nhiều state-of-the-art algorithms đều có elements của cả hai approaches. Key là biết khi nào trust model và khi nào không!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
