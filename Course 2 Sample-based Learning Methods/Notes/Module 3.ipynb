{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference Learning\n",
    "\n",
    "## Khái niệm cơ bản\n",
    "\n",
    "TD learning là **trung tâm** của RL - cho phép cập nhật giá trị từng bước mà không cần chờ episode kết thúc.\n",
    "\n",
    "### TD Error (δₜ)\n",
    "- Là sự khác biệt giữa giá trị ước tính hiện tại và giá trị state tiếp theo\n",
    "- Công thức cập nhật:\n",
    "\n",
    "```\n",
    "V(Sₜ) ← V(Sₜ) + α[Gₜ - V(Sₜ)]\n",
    "```\n",
    "\n",
    "Trong đó:\n",
    "- `Gₜ ≈ Rₜ₊₁ + γV(Sₜ₊₁)` (bootstrap estimate)\n",
    "- `δₜ = Rₜ₊₁ + γV(Sₜ₊₁) - V(Sₜ)` (TD error)\n",
    "\n",
    "**⚠️ Lưu ý**: TD dùng estimate để cập nhật estimate khác - đây là điểm khác biệt chính!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm: Tabular TD(0)\n",
    "\n",
    "**Mục tiêu**: Ước tính value function vᵖ cho policy π\n",
    "\n",
    "**Input**: \n",
    "- Policy π cần evaluate\n",
    "- Learning rate α ∈ (0,1]\n",
    "\n",
    "**Thuật toán**:\n",
    "\n",
    "1. **Khởi tạo**: V(s) random cho tất cả states, V(terminal) = 0\n",
    "\n",
    "2. **Mỗi episode**:\n",
    "   - Khởi tạo state S\n",
    "   - **Mỗi step**:\n",
    "     - Chọn action A theo policy π\n",
    "     - Thực hiện A, nhận reward R và next state S'\n",
    "     - **Cập nhật**: `V(S) ← V(S) + α[R + γV(S') - V(S)]`\n",
    "     - S ← S'\n",
    "   - Lặp đến khi gặp terminal state\n",
    "\n",
    "**Điểm quan trọng**: Cập nhật ngay lập tức sau mỗi step, không cần chờ episode kết thúc!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So sánh 3 phương pháp\n",
    "\n",
    "| Tiêu chí | Monte Carlo | Dynamic Programming | Temporal Difference |\n",
    "|----------|-------------|---------------------|--------------------|\n",
    "| **Model cần thiết** | ❌ Không | ✅ Cần model đầy đủ | ❌ Không |\n",
    "| **Khi nào update** | Cuối episode | Iterate qua toàn bộ state space | Từng step trong episode |\n",
    "| **Continuous tasks** | ❌ Không phù hợp | ✅ OK | ✅ OK |\n",
    "| **Real-time** | ❌ Phải chờ | ❌ Không | ✅ Có |\n",
    "| **Tốc độ học** | Chậm (dùng full return) | Nhanh nếu model chính xác | Nhanh hơn (bootstrap) |\n",
    "| **Sample efficiency** | Kém hơn | Cao (cần full sweep) | Hiệu quả hơn |\n",
    "| **Bias/Variance** | Low bias, high variance | Low variance, higher bias | Moderate cả 2 |\n",
    "| **Scalability** | Trung bình | Kém với large state space | Cao |\n",
    "| **Flexibility** | Hoạt động với unknown env | Chỉ với known env | Hoạt động với unknown env |\n",
    "| **Phản ứng với thay đổi** | Chậm adapt | Phụ thuộc full env update | Nhanh adapt từng step |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ưu điểm chính của TD\n",
    "\n",
    "TD learning **kết hợp** những ý tưởng hay nhất từ MC và DP:\n",
    "\n",
    "- **Từ MC**: Học trực tiếp từ experience, không cần model\n",
    "- **Từ DP**: Cập nhật dựa trên estimates hiện tại (bootstrapping)\n",
    "\n",
    "### Tại sao TD hiệu quả?\n",
    "\n",
    "1. **Học online**: Không cần chờ episode kết thúc\n",
    "2. **Bootstrap**: Dùng current estimates để cải thiện estimates\n",
    "3. **Balance**: Vừa đủ bias và variance\n",
    "4. **Flexible**: Hoạt động với cả finite và continuing tasks\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
